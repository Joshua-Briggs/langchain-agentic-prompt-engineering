{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØüìå **LangChain** Agentic Prompt Engineering Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the **ChatPromptTemplate** üìã\n",
    "\n",
    "Key Components of a **`ChatPromptTemplate`**:\n",
    "\n",
    "- **`System Prompts`**: Defines the AI's overall behavior and role ü§ñ\n",
    "- **`User Prompts`**: provide specific instructions or questions for a particular task or interaction üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "Hey, can you tell me about what Dubai is planning to build for the Dubai Creek area?\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant AI, answer the user's query...\n",
    "Use a {character_voice} tone and language for the response...\n",
    "If you do not know the answer, respond with \"I don't know\"...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **LangChain** we can import the ``ChatPromptTemplate`` ~ this allows us to pass in both a `system` prompt and a `user` prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# passing the template to the LangChain model\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{user_query}\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Down Our Prompt Template üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as you can see we haven't passed our `user_prompt` in just yet, that is because I want to show you the `input_variables`, these tell the developer what is needed to run this prompt, and as you can see we need both a `character_voice` such as a pirate, and a `user_query` such as our `user_prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['character_voice', 'user_query']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will switch the `user_query` input variable with our `user_prompt` to showcase how to view the messages inside of our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the template to the LangChain model\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", user_prompt)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to know what is inside each prompt template, so to do this we can access the `messages`, which has both the system and user prompt template, [0] is the system, and [1] is the user template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant AI, answer the user's query...\n",
      "Use a {character_voice} tone and language for the response...\n",
      "If you do not know the answer, respond with \"I don't know\"...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access the first message in the list, which is the system message\n",
    "system_message_template = prompt_template.messages[0]\n",
    "\n",
    "# Access the 'template' attribute of the system message\n",
    "system_template_content = system_message_template.prompt.template\n",
    "\n",
    "# Print or use the system template content\n",
    "print(system_template_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey, can you tell me about what Dubai is planning to build for the Dubai Creek area?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access the first message in the list, which is the system message\n",
    "user_message_template = prompt_template.messages[1]\n",
    "\n",
    "# Access the 'template' attribute of the system message\n",
    "user_template_content = user_message_template.prompt.template\n",
    "\n",
    "# Print or use the system template content\n",
    "print(user_template_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë Setting Up OpenAI API Key\n",
    "\n",
    "Before we begin using Semantic Router, you'll need to set up your `OpenAI API key`. This key is required for generating `embeddings` that power our semantic routing system.\n",
    "\n",
    "#### üåê Getting Your API Key\n",
    "\n",
    "1. Visit [OpenAI API Keys Page](https://platform.openai.com/api-keys)\n",
    "2. Sign in or create an account if you haven't already\n",
    "3. Click on \"Create new secret key\"\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: New users get `$5 in free credits`. After that, you'll need to set up billing to continue using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"Enter OpenAI API Key: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that `temperature` is also crucial to how you want your application to respond:\n",
    "\n",
    "- If you want a more **creative** LLM, I would suggest going higher towards **1.0**, however this can cause **hallucinations**.\n",
    "- If you would prefer a more **strict** and **context accurate** LLM, I would set the temperature at **0.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.3, model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our **Pipeline** üîÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to apply our `prompt` into this LLM, and to do this we will use `LCEL`, **LangChain**'s unique expression language, this comes with the pipe operator `|` - this just ***feeds the output of the left-most varibale*** into the ***input of the right-most variable***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can `invoke` the pipeline, however this will require any input variables to be defined, as we switched out our `user_prompt` around earlier we only need to apply a `character_voice` as the question is already defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['character_voice']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input these variables as a `dict` as that is what we defined it as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr matey, Dubai be plannin' a grand project for the Dubai Creek area, it be called the Dubai Creek Harbour. This be a massive venture, larger than Downtown Dubai, it be! It's set to include the Dubai Creek Tower, which they be plannin' to make taller than the Burj Khalifa, the current tallest structure in the world. The harbour will also be home to a nature sanctuary, luxury residences, and plenty of spots for shopping and dining. But remember, me hearty, plans can change as swiftly as the sea, so keep ye eyes peeled for updates.\n"
     ]
    }
   ],
   "source": [
    "response = pipeline.invoke({\"character_voice\": \"pirate\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go, this notebook gives you a generic idea of how to use prompting, and in future updates this Repo will have additional notebooks going over the different forms of prompting, this is just an introduction, hope you learned something new and happy coding! üß™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
